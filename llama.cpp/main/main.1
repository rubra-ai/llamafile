.Dd October 12, 2024
.Dt LLAMAFILE 1
.Os Mozilla Ocho
.Sh NAME
.Nm llamafile
.Nd large language model runner
.Sh SYNOPSIS
.Nm
.Op Fl Fl chat
.Op flags...
.Fl m Ar model.gguf
.Nm
.Op Fl Fl server
.Op flags...
.Fl m Ar model.gguf
.Op Fl Fl mmproj Ar vision.gguf
.Nm
.Op Fl Fl cli
.Op flags...
.Fl m Ar model.gguf
.Fl p Ar prompt
.Nm
.Op Fl Fl cli
.Op flags...
.Fl m Ar model.gguf
.Fl Fl mmproj Ar vision.gguf
.Fl Fl image Ar graphic.png
.Fl p Ar prompt
.Sh DESCRIPTION
.Nm
is a large language model tool. It has use cases such as:
.Pp
.Bl -dash -compact
.It
Code completion
.It
Prose composition
.It
Chatbot that passes the Turing test
.It
Text/image summarization and analysis
.El
.Sh MODES
.Pp
There's three modes of operation:
.Fl Fl chat ,
.Fl Fl server ,
and
.Fl Fl cli .
If none of these flags is specified, then llamafile makes its best guess
about which mode is best. By default, the
.Fl Fl chat
interface is launched in the foreground with a
.Fl Fl server
in the background.
.Bl -tag -width indent
.It Fl Fl cli
Puts program in command line interface mode. This flag is implied when a
prompt is supplied using either the
.Fl p
or
.Fl f
flags.
.It Fl Fl chat
Puts program in command line chatbot only mode. This mode launches an
interactive shell that lets you talk to your LLM, which should be
specified using the
.Fl m
flag. This mode also launches a server in the background. The system
prompt that's displayed at the start of your conversation may be changed
by passing the
.Fl p
flag.
.It Fl Fl server
Puts program in server only mode. This will launch an HTTP server on a
local port. This server has both a web UI and an OpenAI API compatible
completions endpoint. When the server is run on a desk system, a tab
browser tab will be launched automatically that displays the web UI.
This
.Fl Fl server
flag is implied if no prompt is specified, i.e. neither the
.Fl p
or
.Fl f
flags are passed.
.El
.Sh OPTIONS
.Pp
The following options are available:
.Bl -tag -width indent
.It Fl Fl version
Print version and exit.
.It Fl h , Fl Fl help
Show help message and exit.
.It Fl m Ar FNAME , Fl Fl model Ar FNAME
Model path in the GGUF file format.
.Pp
Default:
.Pa models/7B/ggml-model-f16.gguf
.It Fl Fl mmproj Ar FNAME
Specifies path of the LLaVA vision model in the GGUF file format. If
this flag is supplied, then the
.Fl Fl model
and
.Fl Fl image
flags should also be supplied.
.It Fl ngl Ar N , Fl Fl n-gpu-layers Ar N
Enables GPU by specifying number of layers to store in VRAM.
.Pp
By default, llamafile runs in CPU mode. The only exception is Apple
Metal, which is reliable enough to be enabled by default. So if you have
an NVIDIA or AMD GPU in your system, then you need to pass this flag to
enable GPU support. The simplest way to do this is to say:
.Pp
.Dl "llamafile -ngl 999 -m model.gguf"
.Pp
Which will cause llamafile to offload as many layers to the GPU as
possible. If you get an out of memory error, then you may tune this to a
smaller number, e.g. 10, to ask llamafile to use both CPU and GPU when
running your model.
.It Fl Fl gpu Ar GPU
Specifies which brand of GPU should be used. Valid choices are:
.Pp
.Bl -dash
.It
.Ar AUTO :
Use any GPU if possible, otherwise fall back to CPU inference
.It
.Ar APPLE :
Use Apple Metal GPU. This is only available on MacOS ARM64. If Metal
could not be used for any reason, then a fatal error will be raised.
.It
.Ar AMD :
Use AMD GPUs. The AMD HIP ROCm SDK should be installed in which case we
assume the HIP_PATH environment variable has been defined. The set of
gfx microarchitectures needed to run on the host machine is determined
automatically based on the output of the hipInfo command. On Windows,
.Nm
release binaries are distributed with a tinyBLAS DLL so it'll work out
of the box without requiring the HIP SDK to be installed. However,
tinyBLAS is slower than rocBLAS for batch and image processing, so it's
recommended that the SDK be installed anyway. If an AMD GPU could not be
used for any reason, then a fatal error will be raised.
.It
.Ar NVIDIA :
Use NVIDIA GPUs. If an NVIDIA GPU could not be used for any reason, a
fatal error will be raised. On Windows, NVIDIA GPU support will use our
tinyBLAS library, since it works on stock Windows installs. However,
tinyBLAS goes slower for batch and image processing. It's possible to
use NVIDIA's closed-source cuBLAS library instead. To do that, both MSVC
and CUDA need to be installed and the
.Nm
command should be run once from the x64 MSVC command prompt with the
.Fl Fl recompile
flag passed. The GGML library will then be compiled and saved to
.Pa ~/.llamafile/
so the special process only needs to happen a single time.
.It
.Ar DISABLE :
Never use GPU and instead use CPU inference. This setting is implied by
.Fl ngl Ar 0 .
.El
.Pp
This flag is useful on systems that have multiple kinds of GPUs. For
example, if you have two graphics cards in your computer, one being AMD
and the other is NVIDIA, then you can use this flag to force llamafile
to use a particular brand.
.Pp
This flag is also useful for preventing CPU fallback. For example, if
you pass
.Fl Fl gpu Ar metal
and llamafile is running on a PC with an NVIDIA card, then the process
will print an error and exit.
.Pp
The default behavior is
.Ar AUTO
however it should be noted that GPU support isn't enabled by default,
since the
.Fl ngl
flag is normally used to enable GPU offloading. As a convenience, if the
.Fl Fl gpu
flag is explicitly passed on the command line, and it's set to
.Ar AUTO ,
.Ar AMD ,
.Ar APPLE ,
or
.Ar NVIDIA ,
but the
.Fl ngl
flag is not passed, then the number of GPU layers will be automatically
set to 999.
.It Fl s Ar SEED , Fl Fl seed Ar SEED
Random Number Generator (RNG) seed. A random seed is used if this is
less than zero.
.Pp
Default: -1
.It Fl t Ar N , Fl Fl threads Ar N
Number of threads to use during generation.
.Pp
Default: $(nproc)/2 max 20
.It Fl tb Ar N , Fl Fl threads-batch Ar N
Number of threads to use during prompt processing.
.Pp
Default: $(nproc)/2
.It Fl c Ar N , Fl Fl ctx-size Ar N
Sets the maximum context size, in tokens. In
.Fl Fl chat
mode, this value sets a hard limit on how long your conversation can be.
The default is 8192 tokens. If this value is zero, then it'll be set to
the maximum context size the model allows.
.It Fl b Ar N , Fl Fl batch-size Ar N
Set batch size for prompt processing.
.Pp
Default: 2048
.It Fl Fl top-k Ar N
Limits next token selection to K most probable tokens.
.Pp
Top-k sampling is a text generation method that selects the next token
only from the top k most likely tokens predicted by the model. It helps
reduce the risk of generating low-probability or nonsensical tokens, but
it may also limit the diversity of the output. A higher value for top-k
(e.g., 100) will consider more tokens and lead to more diverse text,
while a lower value (e.g., 10) will focus on the most probable tokens
and generate more conservative text.
.Pp
Default: 40
.It Fl Fl top-p Ar N
Limits next token selection to a subset of tokens with a cumulative
probability above a threshold P.
.Pp
Top-p sampling, also known as nucleus sampling, is another text
generation method that selects the next token from a subset of tokens
that together have a cumulative probability of at least p. This method
provides a balance between diversity and quality by considering both the
probabilities of tokens and the number of tokens to sample from. A
higher value for top-p (e.g., 0.95) will lead to more diverse text,
while a lower value (e.g., 0.5) will generate more focused and
conservative text.
.Pp
Default: 0.9
.It Fl Fl min-p Ar N
Sets minimum base probability threshold for token selection.
.Pp
The Min-P sampling method was designed as an alternative to Top-P, and
aims to ensure a balance of quality and variety. The parameter p
represents the minimum probability for a token to be considered,
relative to the probability of the most likely token. For example, with
p=0.05 and the most likely token having a probability of 0.9, logits
with a value less than 0.045 are filtered out.
.Pp
Default: 0.05
.It Fl Fl tfs Ar N
Enables tail free sampling with parameter z.
.Pp
Tail free sampling (TFS) is a text generation technique that aims to
reduce the impact of less likely tokens, which may be less relevant,
less coherent, or nonsensical, on the output. Similar to Top-P it tries
to determine the bulk of the most likely tokens dynamically. But TFS
filters out logits based on the second derivative of their
probabilities. Adding tokens is stopped after the sum of the second
derivatives reaches the parameter z. In short: TFS looks how quickly the
probabilities of the tokens decrease and cuts off the tail of unlikely
tokens using the parameter z. Typical values for z are in the range of
0.9 to 0.95. A value of 1.0 would include all tokens, and thus disables
the effect of TFS.
.Pp
Default: 1.0 (which means disabled)
.It Fl Fl typical Ar N
Enables locally typical sampling with parameter p.
.Pp
Locally typical sampling promotes the generation of contextually
coherent and diverse text by sampling tokens that are typical or
expected based on the surrounding context. By setting the parameter p
between 0 and 1, you can control the balance between producing text that
is locally coherent and diverse. A value closer to 1 will promote more
contextually coherent tokens, while a value closer to 0 will promote
more diverse tokens. A value equal to 1 disables locally typical
sampling.
.Pp
Default: 1.0 (which means disabled)
.It Fl Fl repeat-penalty Ar N
Controls repetition of token sequences in generated text.
.Pp
This can help prevent the model from generating repetitive or monotonous
text. A higher value (e.g., 1.5) will penalize repetitions more
strongly, while a lower value (e.g., 0.9) will be more lenient.
.Pp
Default: 1.1
.It Fl Fl repeat-last-n Ar N
Last n tokens to consider for penalizing repetition.
.Pp
This controls the number of tokens in the history to consider for
penalizing repetition. A larger value will look further back in the
generated text to prevent repetitions, while a smaller value will only
consider recent tokens. A value of 0 disables the penalty, and a value
of -1 sets the number of tokens considered equal to the context size.
.Pp
.Bl -dash -compact
.It
0 = disabled
.It
-1 = ctx_size
.El
.Pp
Default: 64
.It Fl Fl no-penalize-nl
Disables penalization of newline tokens when applying the repeat
penalty.
.Pp
This option is particularly useful for generating chat conversations,
dialogues, code, poetry, or any text where newline tokens play a
significant role in structure and formatting. Disabling newline
penalization helps maintain the natural flow and intended formatting in
these specific use cases.
.It Fl Fl presence-penalty Ar N
Repeat alpha presence penalty.
.Pp
.Bl -dash -compact
.It
0.0 = disabled
.El
.Pp
Default: 0.0
.It Fl Fl frequency-penalty Ar N
Repeat alpha frequency penalty.
.Pp
.Bl -dash -compact
.It
0.0 = disabled
.El
.Pp
Default: 0.0
.It Fl Fl mirostat Ar N
Use Mirostat sampling.
.Pp
Mirostat is an algorithm that actively maintains the quality of
generated text within a desired range during text generation. It aims to
strike a balance between coherence and diversity, avoiding low-quality
output caused by excessive repetition (boredom traps) or incoherence
(confusion traps).
.Pp
Using Mirostat causes the Top K, Nucleus, Tail Free and Locally Typical
samplers parameter to be ignored if used.
.Pp
.Bl -dash -compact
.It
0 = disabled
.It
1 = Mirostat
.It
2 = Mirostat 2.0
.El
.Pp
Default: 0
.It Fl Fl mirostat-lr Ar N
Sets the Mirostat learning rate (eta).
.Pp
The learning rate influences how quickly the algorithm responds to
feedback from the generated text. A lower learning rate will result in
slower adjustments, while a higher learning rate will make the algorithm
more responsive.
.Pp
Default: 0.1
.It Fl Fl mirostat-ent Ar N
Sets the Mirostat target entropy (tau).
.Pp
This represents the desired perplexity value for the generated text.
Adjusting the target entropy allows you to control the balance between
coherence and diversity in the generated text. A lower value will result
in more focused and coherent text, while a higher value will lead to
more diverse and potentially less coherent text.
.Pp
Default: 5.0
.It Fl l Ar TOKEN_ID(+/-)BIAS , Fl Fl logit-bias Ar TOKEN_ID(+/-)BIAS
Modifies the likelihood of token appearing in the completion, i.e.
.Fl Fl logit-bias Ar 15043+1
to increase likelihood of token
.Ar ' Hello' ,
or
.Fl Fl logit-bias Ar 15043-1
to decrease likelihood of token
.Ar ' Hello' .
.It Fl Fl cfg-negative-prompt Ar PROMPT
Negative prompt to use for guidance..
.Pp
Default: empty
.It Fl Fl cfg-negative-prompt-file Ar FNAME
Negative prompt file to use for guidance.
.Pp
Default: empty
.It Fl Fl cfg-scale Ar N
Strength of guidance.
.Pp
.Bl -dash -compact
.It
1.0 = disable
.El
.Pp
Default: 1.0
.It Fl Fl rope-scaling Ar {none,linear,yarn}
RoPE frequency scaling method, defaults to linear unless specified by the model
.It Fl Fl rope-scale Ar N
RoPE context scaling factor, expands context by a factor of
.Ar N
where
.Ar N
is the linear scaling factor used by the fine-tuned model. Some
fine-tuned models have extended the context length by scaling RoPE. For
example, if the original pre-trained model have a context length (max
sequence length) of 4096 (4k) and the fine-tuned model have 32k. That is
a scaling factor of 8, and should work by setting the above
.Fl Fl ctx-size
to 32768 (32k) and
.Fl Fl rope-scale
to 8.
.It Fl Fl rope-freq-base Ar N
RoPE base frequency, used by NTK-aware scaling.
.Pp
Default: loaded from model
.It Fl Fl rope-freq-scale Ar N
RoPE frequency scaling factor, expands context by a factor of 1/N
.It Fl Fl yarn-orig-ctx Ar N
YaRN: original context size of model.
.Pp
Default: 0 = model training context size
.It Fl Fl yarn-ext-factor Ar N
YaRN: extrapolation mix factor.
.Pp
.Bl -dash -compact
.It
0.0 = full interpolation
.El
.Pp
Default: 1.0
.It Fl Fl yarn-attn-factor Ar N
YaRN: scale sqrt(t) or attention magnitude.
.Pp
Default: 1.0
.It Fl Fl yarn-beta-slow Ar N
YaRN: high correction dim or alpha.
.Pp
Default: 1.0
.It Fl Fl yarn-beta-fast Ar N
YaRN: low correction dim or beta.
.Pp
Default: 32.0
.It Fl Fl temp Ar N
Adjust the randomness of the generated text.
.Pp
Temperature is a hyperparameter that controls the randomness of the
generated text. It affects the probability distribution of the model's
output tokens. A higher temperature (e.g., 1.5) makes the output more
random and creative, while a lower temperature (e.g., 0.5) makes the
output more focused, deterministic, and conservative. The default value
is 0.8, which provides a balance between randomness and determinism. At
the extreme, a temperature of 0 will always pick the most likely next
token, leading to identical outputs in each run.
.Pp
Default: 0.8 in cli and server mode, and 0.0 in chat mode
.It Fl Fl logits-all
Return logits for all tokens in the batch.
.Pp
Default: disabled
.It Fl ns Ar N , Fl Fl sequences Ar N
Number of sequences to decode.
.Pp
Default: 1
.It Fl pa Ar N , Fl Fl p-accept Ar N
speculative decoding accept probability.
.Pp
Default: 0.5
.It Fl ps Ar N , Fl Fl p-split Ar N
Speculative decoding split probability.
.Pp
Default: 0.1
.It Fl Fl mlock
Force system to keep model in RAM rather than swapping or compressing.
.It Fl Fl no-mmap
Do not memory-map model (slower load but may reduce pageouts if not using mlock).
.It Fl Fl numa
Attempt optimizations that help on some NUMA systems if run without this previously, it is recommended to drop the system page cache before using this. See https://github.com/ggerganov/llama.cpp/issues/1437.
.It Fl Fl recompile
Force GPU support to be recompiled at runtime if possible.
.It Fl Fl nocompile
Never compile GPU support at runtime.
.Pp
If the appropriate DSO file already exists under
.Pa ~/.llamafile/
then it'll be linked as-is without question. If a prebuilt DSO is
present in the PKZIP content of the executable, then it'll be extracted
and linked if possible. Otherwise,
.Nm
will skip any attempt to compile GPU support and simply fall back to
using CPU inference.
.It Fl sm Ar SPLIT_MODE , Fl Fl split-mode Ar SPLIT_MODE
How to split the model across multiple GPUs, one of:
.Bl -dash -compact
.It
none: use one GPU only
.It
layer (default): split layers and KV across GPUs
.It
row: split rows across GPUs
.El
.It Fl ts Ar SPLIT , Fl Fl tensor-split Ar SPLIT
When using multiple GPUs this option controls how large tensors should
be split across all GPUs.
.Ar SPLIT
is a comma-separated list of non-negative values that assigns the
proportion of data that each GPU should get in order. For example,
\[dq]3,2\[dq] will assign 60% of the data to GPU 0 and 40% to GPU 1. By
default the data is split in proportion to VRAM but this may not be
optimal for performance. Requires cuBLAS.
How to split tensors across multiple GPUs, comma-separated list of
proportions, e.g. 3,1
.It Fl mg Ar i , Fl Fl main-gpu Ar i
The GPU to use for scratch and small tensors.
.It Fl Fl verbose-prompt
Print prompt before generation.
.It Fl Fl lora Ar FNAME
Apply LoRA adapter (implies
.Fl Fl no-mmap )
.It Fl Fl lora-scaled Ar FNAME Ar S
Apply LoRA adapter with user defined scaling S (implies
.Fl Fl no-mmap )
.It Fl Fl lora-base Ar FNAME
Optional model to use as a base for the layers modified by the LoRA adapter
.It Fl Fl unsecure
Disables pledge() sandboxing on Linux and OpenBSD.
.It Fl Fl samplers
Samplers that will be used for generation in the order, separated by
semicolon, for example: top_k;tfs;typical;top_p;min_p;temp
.It Fl Fl samplers-seq
Simplified sequence for samplers that will be used.
.It Fl dkvc , Fl Fl dump-kv-cache
Verbose print of the KV cache.
.It Fl nkvo , Fl Fl no-kv-offload
Disable KV offload.
.It Fl gan Ar N , Fl Fl grp-attn-n Ar N
Group-attention factor.
.Pp
Default: 1
.It Fl gaw Ar N , Fl Fl grp-attn-w Ar N
Group-attention width.
.Pp
Default: 512
.It Fl bf Ar FNAME , Fl Fl binary-file Ar FNAME
Binary file containing multiple choice tasks.
.It Fl Fl multiple-choice
Compute multiple choice score over random tasks from datafile supplied
by the
.Fl f
flag.
.It Fl Fl multiple-choice-tasks Ar N
Number of tasks to use when computing the multiple choice score.
.Pp
Default: 0
.It Fl Fl kl-divergence
Computes KL-divergence to logits provided via the
.Fl Fl kl-divergence-base
flag.
.It Fl Fl save-all-logits Ar FNAME , Fl Fl kl-divergence-base Ar FNAME
Save logits to filename.
.It Fl ptc Ar N , Fl Fl print-token-count Ar N
Print token count every
.Ar N
tokens.
.Pp
Default: -1
.It Fl Fl pooling Ar KIND
Specifies pooling type for embeddings. This may be one of:
.Pp
.Bl -dash -compact
.It
none
.It
mean
.It
cls
.El
.Pp
The model default is used if unspecified.
.El
.Sh CHAT OPTIONS
The following options may be specified when
.Nm
is running in
.Fl Fl chat
mode.
.Bl -tag -width indent
.It Fl p Ar STRING , Fl Fl prompt Ar STRING
Specifies system prompt.
.Pp
The system prompt is used to give instructions to the LLM at the
beginning of the conversation. For many model architectures, this is
done under a special role. The system prompt also gets special treatment
when managing the context window. For example, the /clear command will
erase everything except the system prompt, and the /forget command will
erase the oldest chat message that isn't the system prompt.
.Pp
For example:
.Pp
.Dl "llamafile --chat -m model.gguf -p \[dq]You are Mosaic's Godzilla.\[dq]"
.Pp
may be used to instruct your llamafile to roleplay as Mozilla.
.It Fl f Ar FNAME , Fl Fl file Ar FNAME
Uses content of file as system prompt.
.It Fl Fl no-display-prompt , Fl Fl silent-prompt
Suppress printing of system prompt at beginning of conversation.
.It Fl Fl nologo
Disables printing the llamafile logo during chatbot startup.
.It Fl Fl ascii
This flag may be used in
.Fl Fl chat
mode to print the llamafile logo in ASCII rather than UNICODE.
.It Fl Fl verbose
Enables verbose logger output in chatbot. This can be helpful for
troubleshooting issues.
.It Fl Fl chat-template Ar NAME
Specifies or overrides chat template for model.
.Pp
Normally the GGUF metadata tokenizer.chat_template will specify this
value for instruct models. This flag may be used to either override the
chat template, or specify one when the GGUF metadata field is absent,
which effectively forces the web ui to enable chatbot mode.
.Pp
Supported chat template names are: chatml, llama2, llama3, mistral
(alias for llama2), phi3, zephyr, monarch, gemma, gemma2 (alias for
gemma), orion, openchat, vicuna, vicuna-orca, deepseek, command-r,
chatglm3, chatglm4, minicpm, deepseek2, or exaone3.
.Pp
It is also possible to pass the jinja2 template itself to this argument.
Since llamafiler doesn't currently support jinja2, a heuristic will be
used to guess which of the above templates the template represents.
.El
.Sh CLI OPTIONS
The following options may be specified when
.Nm
is running in
.Fl Fl cli
mode.
.Bl -tag -width indent
.It Fl p Ar STRING , Fl Fl prompt Ar STRING
Prompt to start text generation. Your LLM works by auto-completing this
text. For example:
.Pp
.Dl "llamafile -m model.gguf -p \[dq]four score and\[dq]"
.Pp
Stands a pretty good chance of printing Lincoln's Gettysburg Address.
Prompts can take on a structured format too. Depending on how your model
was trained, it may specify in its docs an instruction notation. With
some models that might be:
.Pp
.Dl "llamafile -p \[dq][INST]Summarize this: $(cat file)[/INST]\[dq]"
.Pp
In most cases, simply colons and newlines will work too:
.Pp
.Dl "llamafile -e -p \[dq]User: What is best in life?\[rs]nAssistant:\[dq]"
.Pp
.It Fl f Ar FNAME , Fl Fl file Ar FNAME
Prompt file to start generation.
.It Fl n Ar N , Fl Fl n-predict Ar N
Sets number of tokens to predict when generating text.
.Pp
This option controls the number of tokens the model generates in
response to the input prompt. By adjusting this value, you can influence
the length of the generated text. A higher value will result in longer
text, while a lower value will produce shorter text.
.Pp
A value of -1 will enable infinite text generation, even though we have
a finite context window. When the context window is full, some of the
earlier tokens (half of the tokens after
.Fl Fl n-keep )
will be discarded. The context must then be re-evaluated before
generation can resume. On large models and/or large context windows,
this will result in significant pause in output.
.Pp
If the pause is undesirable, a value of -2 will stop generation
immediately when the context is filled.
.Pp
It is important to note that the generated text may be shorter than the
specified number of tokens if an End-of-Sequence (EOS) token or a
reverse prompt is encountered. In interactive mode text generation will
pause and control will be returned to the user. In non-interactive mode,
the program will end. In both cases, the text generation may stop before
reaching the specified `n-predict` value. If you want the model to keep
going without ever producing End-of-Sequence on its own, you can use the
.Fl Fl ignore-eos
parameter.
.Pp
.Bl -dash -compact
.It
-1 = infinity
.It
-2 = until context filled
.El
.Pp
Default: -1
.It Fl Fl simple-io
Use basic IO for better compatibility in subprocesses and limited consoles.
.It Fl cml , Fl Fl chatml
Run in chatml mode (use with ChatML-compatible models)
.It Fl e , Fl Fl escape
Process prompt escapes sequences (\[rs]n, \[rs]r, \[rs]t, \[rs]\[aa], \[rs]\[dq], \[rs]\[rs])
.It Fl Fl grammar Ar GRAMMAR
BNF-like grammar to constrain which tokens may be selected when
generating text. For example, the grammar:
.Pp
.Dl "root ::= \[dq]yes\[dq] | \[dq]no\[dq]"
.Pp
will force the LLM to only output yes or no before exiting. This is
useful for shell scripts when the
.Fl Fl no-display-prompt
flag is also supplied.
.It Fl Fl grammar-file Ar FNAME
File to read grammar from.
.It Fl Fl fast
Put llamafile into fast math mode. This disables algorithms that reduce
floating point rounding, e.g. Kahan summation, and certain functions
like expf() will be vectorized but handle underflows less gracefully.
It's unspecified whether llamafile runs in fast or precise math mode
when neither flag is specified.
.It Fl Fl precise
Put llamafile into precise math mode. This enables algorithms that
reduce floating point rounding, e.g. Kahan summation, and certain
functions like expf() will always handle subnormals correctly. It's
unspecified whether llamafile runs in fast or precise math mode when
neither flag is specified.
.It Fl Fl trap
Put llamafile into math trapping mode. When floating point exceptions
occur, such as NaNs, overflow, and divide by zero, llamafile will print
a warning to the console. This warning will include a C++ backtrace the
first time an exception is trapped. The op graph will also be dumped to
a file, and llamafile will report the specific op where the exception
occurred. This is useful for troubleshooting when reporting issues.
USing this feature will disable sandboxing. Math trapping is only
possible if your CPU supports it. That is generally the case on AMD64,
however it's less common on ARM64.
.It Fl Fl prompt-cache Ar FNAME
File to cache prompt state for faster startup.
.Pp
Default: none
.It Fl fa Ar FNAME , Fl Fl flash-attn
Enable Flash Attention. This is a mathematical shortcut that can speed
up inference for certain models. This feature is still under active
development.
.It Fl Fl prompt-cache-all
If specified, saves user input and generations to cache as well. Not supported with
.Fl Fl interactive
or other interactive options.
.It Fl Fl prompt-cache-ro
If specified, uses the prompt cache but does not update it.
.It Fl Fl random-prompt
Start with a randomized prompt.
.It Fl Fl image Ar IMAGE_FILE
Path to an image file. This should be used with multimodal models.
Alternatively, it's possible to embed an image directly into the prompt
instead; in which case, it must be base64 encoded into an HTML img tag
URL with the image/jpeg MIME type. See also the
.Fl Fl mmproj
flag for supplying the vision model.
.It Fl i , Fl Fl interactive
Run the program in interactive mode, allowing users to engage in
real-time conversations or provide specific instructions to the model.
.It Fl Fl interactive-first
Run the program in interactive mode and immediately wait for user input
before starting the text generation.
.It Fl ins , Fl Fl instruct
Run the program in instruction mode, which is specifically designed to
work with Alpaca models that excel in completing tasks based on user
instructions.
.Pp
Technical details: The user's input is internally prefixed with the
reverse prompt (or \[dq]### Instruction:\[dq] as the default), and
followed by \[dq]### Response:\[dq] (except if you just press Return
without any input, to keep generating a longer response).
.Pp
By understanding and utilizing these interaction options, you can create
engaging and dynamic experiences with the LLaMA models, tailoring the
text generation process to your specific needs.
.It Fl r Ar PROMPT , Fl Fl reverse-prompt Ar PROMPT
Specify one or multiple reverse prompts to pause text generation and
switch to interactive mode. For example,
.Fl r Ar \[dq]User:\[dq]
can be used to jump back into the conversation whenever it's the user's
turn to speak. This helps create a more interactive and conversational
experience. However, the reverse prompt doesn't work when it ends with a
space. To overcome this limitation, you can use the
.Fl Fl in-prefix
flag to add a space or any other characters after the reverse prompt.
.It Fl Fl color
Enable colorized output to differentiate visually distinguishing between
prompts, user input, and generated text.
.It Fl Fl no-display-prompt , Fl Fl silent-prompt
Don't echo the prompt itself to standard output.
.It Fl Fl keep Ar N
Specifies number of tokens to keep from the initial prompt. The default
is -1 which means all tokens.
.It Fl Fl multiline-input
Allows you to write or paste multiple lines without ending each in '\[rs]'.
.It Fl Fl cont-batching
Enables continuous batching, a.k.a. dynamic batching.
is -1 which means all tokens.
.It Fl Fl embedding
In CLI mode, the embedding flag may be use to print embeddings to
standard output. By default, embeddings are computed over a whole
prompt. However the
.Fl Fl multiline
flag may be passed, to have a separate embeddings array computed for
each line of text in the prompt. In multiline mode, each embedding array
will be printed on its own line to standard output, where individual
floats are separated by space. If both the
.Fl Fl multiline-input
and
.Fl Fl interactive
flags are passed, then a pretty-printed summary of embeddings along with
a cosine similarity matrix will be printed to the terminal.
.It Fl Fl ignore-eos
Ignore end of stream token and continue generating (implies
.Fl Fl logit-bias Ar 2-inf )
.It Fl Fl keep Ar N
This flag allows users to retain the original prompt when the model runs
out of context, ensuring a connection to the initial instruction or
conversation topic is maintained, where
.Ar N
is the number of tokens from the initial prompt to retain when the model
resets its internal context.
.Pp
.Bl -dash -compact
.It
0 = no tokens are kept from initial prompt
.It
-1 = retain all tokens from initial prompt
.El
.Pp
Default: 0
.It Fl Fl in-prefix-bos
Prefix BOS to user inputs, preceding the
.Fl Fl in-prefix
string.
.It Fl Fl in-prefix Ar STRING
This flag is used to add a prefix to your input, primarily, this is used
to insert a space after the reverse prompt. Here's an example of how to
use the
.Fl Fl in-prefix
flag in conjunction with the
.Fl Fl reverse-prompt
flag:
.Pp
.Dl "./main -r \[dq]User:\[dq] --in-prefix \[dq] \[dq]"
.Pp
Default: empty
.It Fl Fl in-suffix Ar STRING
This flag is used to add a suffix after your input. This is useful for
adding an \[dq]Assistant:\[dq] prompt after the user's input. It's added
after the new-line character (\[rs]n) that's automatically added to the
end of the user's input. Here's an example of how to use the
.Fl Fl in-suffix
flag in conjunction with the
.Fl Fl reverse-prompt
flag:
.Pp
.Dl "./main -r \[dq]User:\[dq] --in-prefix \[dq] \[dq] --in-suffix \[dq]Assistant:\[dq]"
.Pp
Default: empty
.El
.Sh SERVER OPTIONS
The following options may be specified when
.Nm
is running in
.Fl Fl server
mode.
.Bl -tag -width indent
.It Fl Fl port Ar PORT
Port to listen
.Pp
Default: 8080
.It Fl Fl host Ar IPADDR
IP address to listen.
.Pp
Default: 127.0.0.1
.It Fl to Ar N , Fl Fl timeout Ar N
Server read/write timeout in seconds.
.Pp
Default: 600
.It Fl np Ar N , Fl Fl parallel Ar N
Number of slots for process requests.
.Pp
Default: 1
.It Fl cb , Fl Fl cont-batching
Enable continuous batching (a.k.a dynamic batching).
.Pp
Default: disabled
.It Fl spf Ar FNAME , Fl Fl system-prompt-file Ar FNAME
Set a file to load a system prompt (initial prompt of all slots), this
is useful for chat applications.
.It Fl a Ar ALIAS , Fl Fl alias Ar ALIAS
Set an alias for the model. This will be added as the
.Ar model
field in completion responses.
.It Fl Fl path Ar PUBLIC_PATH
Path from which to serve static files.
.Pp
Default:
.Pa /zip/llama.cpp/server/public
.It Fl Fl url-prefix Ar PREFIX
Specify a URL prefix (subdirectory) under which the API will be served, e.g. /llamafile
.Pp
Default:
.Pa /
.It Fl Fl nobrowser
Do not attempt to open a web browser tab at startup.
.It Fl gan Ar N , Fl Fl grp-attn-n Ar N
Set the group attention factor to extend context size through
self-extend. The default value is
.Ar 1
which means disabled. This flag is used together with
.Fl Fl grp-attn-w .
.It Fl gaw Ar N , Fl Fl grp-attn-w Ar N
Set the group attention width to extend context size through
self-extend. The default value is
.Ar 512 .
This flag is used together with
.Fl Fl grp-attn-n .
.El
.Sh LOG OPTIONS
The following log options are available:
.Bl -tag -width indent
.It Fl ld Ar LOGDIR , Fl Fl logdir Ar LOGDIR
Path under which to save YAML logs (no logging if unset)
.It Fl Fl log-test
Run simple logging test
.It Fl Fl log-disable
Disable trace logs
.It Fl Fl log-enable
Enable trace logs
.It Fl Fl log-file
Specify a log filename (without extension)
.It Fl Fl log-new
Create a separate new log file on start. Each log file will have unique name:
.Fa <name>.<ID>.log
.It Fl Fl log-append
Don't truncate the old log file.
.El
.Sh EXAMPLES
Here's an example of how to run llama.cpp's built-in HTTP server. This
example uses LLaVA v1.5-7B, a multimodal LLM that works with llama.cpp's
recently-added support for image inputs.
.Bd -literal -offset indent
llamafile \[rs]
  -m llava-v1.5-7b-Q8_0.gguf \[rs]
  --mmproj llava-v1.5-7b-mmproj-Q8_0.gguf \[rs]
  --host 0.0.0.0
.Ed
.Pp
Here's an example of how to generate code for a libc function using the
llama.cpp command line interface, utilizing WizardCoder-Python-13B
weights:
.Bd -literal -offset indent
llamafile \[rs]
  -m wizardcoder-python-13b-v1.0.Q8_0.gguf --temp 0 -r '}\[rs]n' -r '\`\`\`\[rs]n' \[rs]
  -e -p '\`\`\`c\[rs]nvoid *memcpy(void *dst, const void *src, size_t size) {\[rs]n'
.Ed
.Pp
Here's a similar example that instead utilizes Mistral-7B-Instruct
weights for prose composition:
.Bd -literal -offset indent
llamafile \[rs]
  -m mistral-7b-instruct-v0.2.Q5_K_M.gguf \[rs]
  -p '[INST]Write a story about llamas[/INST]'
.Ed
.Pp
Here's an example of how llamafile can be used as an interactive chatbot
that lets you query knowledge contained in training data:
.Bd -literal -offset indent
llamafile -m llama-65b-Q5_K.gguf -p '
The following is a conversation between a Researcher and their helpful AI
assistant Digital Athena which is a large language model trained on the
sum of human knowledge.
Researcher: Good morning.
Digital Athena: How can I help you today?
Researcher:' --interactive --color --batch_size 1024 --ctx_size 4096 \[rs]
--keep -1 --temp 0 --mirostat 2 --in-prefix ' ' --interactive-first \[rs]
--in-suffix 'Digital Athena:' --reverse-prompt 'Researcher:'
.Ed
.Pp
Here's an example of how you can use llamafile to summarize HTML URLs:
.Bd -literal -offset indent
(
  echo '[INST]Summarize the following text:'
  links -codepage utf-8 \[rs]
        -force-html \[rs]
        -width 500 \[rs]
        -dump https://www.poetryfoundation.org/poems/48860/the-raven |
    sed 's/   */ /g'
  echo '[/INST]'
) | llamafile \[rs]
      -m mistral-7b-instruct-v0.2.Q5_K_M.gguf \[rs]
      -f /dev/stdin \[rs]
      -c 0 \[rs]
      --temp 0 \[rs]
      -n 500 \[rs]
      --no-display-prompt 2>/dev/null
.Ed
.Pp
Here's how you can use llamafile to describe a jpg/png/gif image:
.Bd -literal -offset indent
llamafile --temp 0 \[rs]
  --image lemurs.jpg \[rs]
  -m llava-v1.5-7b-Q4_K.gguf \[rs]
  --mmproj llava-v1.5-7b-mmproj-Q4_0.gguf \[rs]
  -e -p '### User: What do you see?\[rs]n### Assistant: ' \[rs]
  --no-display-prompt 2>/dev/null
.Ed
.Pp
If you wanted to write a script to rename all your image files, you
could use the following command to generate a safe filename:
.Bd -literal -offset indent
llamafile --temp 0 \[rs]
    --image ~/Pictures/lemurs.jpg \[rs]
    -m llava-v1.5-7b-Q4_K.gguf \[rs]
    --mmproj llava-v1.5-7b-mmproj-Q4_0.gguf \[rs]
    --grammar 'root ::= [a-z]+ (" " [a-z]+)+' \[rs]
    -e -p '### User: The image has...\[rs]n### Assistant: ' \[rs]
    --no-display-prompt 2>/dev/null |
  sed -e's/ /_/g' -e's/$/.jpg/'
three_baby_lemurs_on_the_back_of_an_adult_lemur.jpg
.Ed
.Pp
Here's an example of how to make an API request to the OpenAI API
compatible completions endpoint when your
.Nm
is running in the background in
.Fl Fl server
mode.
.Bd -literal -offset indent
curl -s http://localhost:8080/v1/chat/completions \[rs]
     -H "Content-Type: application/json" -d '{
  "model": "gpt-3.5-turbo",
  "stream": true,
  "messages": [
    {
      "role": "system",
      "content": "You are a poetic assistant."
    },
    {
      "role": "user",
      "content": "Compose a poem that explains FORTRAN."
    }
  ]
}' | python3 -c '
import json
import sys
json.dump(json.load(sys.stdin), sys.stdout, indent=2)
print()
'
.Ed
.Sh PROTIP
The
.Fl ngl Ar 35
flag needs to be passed in order to use GPUs made by NVIDIA and AMD.
It's not enabled by default since it sometimes needs to be tuned based
on the system hardware and model architecture, in order to achieve
optimal performance, and avoid compromising a shared display.
.Sh SEE ALSO
.Xr llamafile-quantize 1 ,
.Xr llamafile-perplexity 1 ,
.Xr llava-quantize 1 ,
.Xr zipalign 1 ,
.Xr unzip 1
