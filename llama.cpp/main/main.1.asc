[4mLLAMAFILE[24m(1)                General Commands Manual               [4mLLAMAFILE[24m(1)

[1mNAME[0m
       llamafile ‚Äî large language model runner

[1mSYNOPSIS[0m
       [1mllamafile [22m[[1m--chat[22m] [flags...] [1m-m [4m[22mmodel.gguf[0m
       [1mllamafile [22m[[1m--server[22m] [flags...] [1m-m [4m[22mmodel.gguf[24m [[1m--mmproj [4m[22mvision.gguf[24m]
       [1mllamafile [22m[[1m--cli[22m] [flags...] [1m-m [4m[22mmodel.gguf[24m [1m-p [4m[22mprompt[0m
       [1mllamafile [22m[[1m--cli[22m] [flags...] [1m-m [4m[22mmodel.gguf[24m [1m--mmproj [4m[22mvision.gguf[24m [1m--image[0m
                 [4mgraphic.png[24m [1m-p [4m[22mprompt[0m

[1mDESCRIPTION[0m
       [1mllamafile [22mis a large language model tool. It has use cases such as:

       [1m-   [22mCode completion
       [1m-   [22mProse composition
       [1m-   [22mChatbot that passes the Turing test
       [1m-   [22mText/image summarization and analysis

[1mMODES[0m
       There's three modes of operation: [1m--chat[22m, [1m--server[22m, and [1m--cli[22m.  If none
       of  these flags is specified, then llamafile makes its best guess about
       which mode is best. By default, the [1m--chat [22minterface is launched in the
       foreground with a [1m--server [22min the background.

       [1m--cli   [22mPuts program in command line interface mode. This flag  is  im‚Äê
               plied  when  a  prompt  is  supplied  using either the [1m-p [22mor [1m-f[0m
               flags.

       [1m--chat  [22mPuts program in command  line  chatbot  only  mode.  This  mode
               launches  an  interactive shell that lets you talk to your LLM,
               which should be specified using the [1m-m  [22mflag.  This  mode  also
               launches  a  server in the background. The system prompt that's
               displayed at the start of your conversation may be  changed  by
               passing the [1m-p [22mflag.

       [1m--server[0m
               Puts  program  in  server  only  mode. This will launch an HTTP
               server on a local port. This server has both a web  UI  and  an
               OpenAI  API compatible completions endpoint. When the server is
               run on a desk system, a tab browser tab will be launched  auto‚Äê
               matically  that displays the web UI.  This [1m--server [22mflag is im‚Äê
               plied if no prompt is specified, i.e.  neither  the  [1m-p  [22mor  [1m-f[0m
               flags are passed.

[1mOPTIONS[0m
       The following options are available:

       [1m--version[0m
               Print version and exit.

       [1m-h[22m, [1m--help[0m
               Show help message and exit.

       [1m-m [4m[22mFNAME[24m, [1m--model [4m[22mFNAME[0m
               Model path in the GGUF file format.

               Default: [4mmodels/7B/ggml-model-f16.gguf[0m

       [1m--mmproj [4m[22mFNAME[0m
               Specifies  path of the LLaVA vision model in the GGUF file for‚Äê
               mat. If this flag is supplied, then  the  [1m--model  [22mand  [1m--image[0m
               flags should also be supplied.

       [1m-ngl [4m[22mN[24m, [1m--n-gpu-layers [4m[22mN[0m
               Enables GPU by specifying number of layers to store in VRAM.

               By  default,  llamafile runs in CPU mode. The only exception is
               Apple Metal, which is reliable enough to be enabled by default.
               So if you have an NVIDIA or AMD GPU in your  system,  then  you
               need  to pass this flag to enable GPU support. The simplest way
               to do this is to say:

                     [1mllamafile -ngl 999 -m model.gguf[0m

               Which will cause llamafile to offload as many layers to the GPU
               as possible. If you get an out of memory error,  then  you  may
               tune this to a smaller number, e.g. 10, to ask llamafile to use
               both CPU and GPU when running your model.

       [1m--gpu [4m[22mGPU[0m
               Specifies which brand of GPU should be used. Valid choices are:

               [1m-   [4m[22mAUTO[24m:  Use  any GPU if possible, otherwise fall back to CPU
                   inference

               [1m-   [4m[22mAPPLE[24m: Use Apple Metal GPU. This is only available on MacOS
                   ARM64. If Metal could not be used for any  reason,  then  a
                   fatal error will be raised.

               [1m-   [4m[22mAMD[24m: Use AMD GPUs. The AMD HIP ROCm SDK should be installed
                   in  which  case we assume the HIP_PATH environment variable
                   has been defined. The set of gfx microarchitectures  needed
                   to  run  on  the  host  machine is determined automatically
                   based on the output of the  hipInfo  command.  On  Windows,
                   [1mllamafile  [22mrelease binaries are distributed with a tinyBLAS
                   DLL so it'll work out of the box without requiring the  HIP
                   SDK  to  be  installed.  However,  tinyBLAS  is slower than
                   rocBLAS for batch and image processing, so it's recommended
                   that the SDK be installed anyway. If an AMD GPU  could  not
                   be used for any reason, then a fatal error will be raised.

               [1m-   [4m[22mNVIDIA[24m: Use NVIDIA GPUs. If an NVIDIA GPU could not be used
                   for  any  reason, a fatal error will be raised. On Windows,
                   NVIDIA GPU support will use our tinyBLAS library, since  it
                   works  on  stock  Windows  installs. However, tinyBLAS goes
                   slower for batch and image processing. It's possible to use
                   NVIDIA's closed-source cuBLAS library instead. To do  that,
                   both  MSVC  and CUDA need to be installed and the [1mllamafile[0m
                   command should be run once from the x64 MSVC command prompt
                   with the [1m--recompile [22mflag passed.  The  GGML  library  will
                   then  be compiled and saved to [4m~/.llamafile/[24m so the special
                   process only needs to happen a single time.

               [1m-   [4m[22mDISABLE[24m: Never use GPU and instead use CPU inference.  This
                   setting is implied by [1m-ngl [4m[22m0[24m.

               This  flag  is  useful  on  systems that have multiple kinds of
               GPUs. For example, if you have two graphics cards in your  com‚Äê
               puter,  one being AMD and the other is NVIDIA, then you can use
               this flag to force llamafile to use a particular brand.

               This flag is also useful for preventing CPU fallback. For exam‚Äê
               ple, if you pass [1m--gpu [4m[22mmetal[24m and llamafile is running on  a  PC
               with  an  NVIDIA card, then the process will print an error and
               exit.

               The default behavior is [4mAUTO[24m however it should  be  noted  that
               GPU  support  isn't  enabled by default, since the [1m-ngl [22mflag is
               normally used to enable GPU offloading. As  a  convenience,  if
               the  [1m--gpu  [22mflag  is explicitly passed on the command line, and
               it's set to [4mAUTO[24m, [4mAMD[24m, [4mAPPLE[24m, or [4mNVIDIA[24m, but the [1m-ngl  [22mflag  is
               not passed, then the number of GPU layers will be automatically
               set to 999.

       [1m-s [4m[22mSEED[24m, [1m--seed [4m[22mSEED[0m
               Random  Number  Generator  (RNG) seed. A random seed is used if
               this is less than zero.

               Default: -1

       [1m-t [4m[22mN[24m, [1m--threads [4m[22mN[0m
               Number of threads to use during generation.

               Default: $(nproc)/2 max 20

       [1m-tb [4m[22mN[24m, [1m--threads-batch [4m[22mN[0m
               Number of threads to use during prompt processing.

               Default: $(nproc)/2

       [1m-c [4m[22mN[24m, [1m--ctx-size [4m[22mN[0m
               Sets the maximum context size, in tokens. In [1m--chat [22mmode,  this
               value  sets  a hard limit on how long your conversation can be.
               The default is 8192 tokens. If this value is zero,  then  it'll
               be set to the maximum context size the model allows.

       [1m-b [4m[22mN[24m, [1m--batch-size [4m[22mN[0m
               Set batch size for prompt processing.

               Default: 2048

       [1m--top-k [4m[22mN[0m
               Limits next token selection to K most probable tokens.

               Top-k  sampling  is  a  text generation method that selects the
               next token only from the top k most likely tokens predicted  by
               the model. It helps reduce the risk of generating low-probabil‚Äê
               ity  or nonsensical tokens, but it may also limit the diversity
               of the output. A higher value for top-k (e.g., 100)  will  con‚Äê
               sider  more tokens and lead to more diverse text, while a lower
               value (e.g., 10) will focus on the  most  probable  tokens  and
               generate more conservative text.

               Default: 40

       [1m--top-p [4m[22mN[0m
               Limits  next token selection to a subset of tokens with a cumu‚Äê
               lative probability above a threshold P.

               Top-p sampling, also known as nucleus sampling, is another text
               generation method that selects the next token from a subset  of
               tokens  that together have a cumulative probability of at least
               p. This method provides a balance between diversity and quality
               by considering both the probabilities of tokens and the  number
               of tokens to sample from. A higher value for top-p (e.g., 0.95)
               will lead to more diverse text, while a lower value (e.g., 0.5)
               will generate more focused and conservative text.

               Default: 0.9

       [1m--min-p [4m[22mN[0m
               Sets minimum base probability threshold for token selection.

               The  Min-P  sampling  method  was designed as an alternative to
               Top-P, and aims to ensure a balance of quality and variety. The
               parameter p represents the minimum probability for a  token  to
               be  considered,  relative to the probability of the most likely
               token. For example, with p=0.05 and the most likely token  hav‚Äê
               ing  a  probability of 0.9, logits with a value less than 0.045
               are filtered out.

               Default: 0.05

       [1m--tfs [4m[22mN[0m
               Enables tail free sampling with parameter z.

               Tail free sampling (TFS) is a text  generation  technique  that
               aims  to  reduce the impact of less likely tokens, which may be
               less relevant, less coherent, or nonsensical,  on  the  output.
               Similar  to  Top-P  it  tries to determine the bulk of the most
               likely tokens dynamically. But TFS filters out logits based  on
               the  second derivative of their probabilities. Adding tokens is
               stopped after the sum of the second derivatives reaches the pa‚Äê
               rameter z. In short: TFS looks how quickly the probabilities of
               the tokens decrease and cuts off the tail  of  unlikely  tokens
               using the parameter z. Typical values for z are in the range of
               0.9  to 0.95. A value of 1.0 would include all tokens, and thus
               disables the effect of TFS.

               Default: 1.0 (which means disabled)

       [1m--typical [4m[22mN[0m
               Enables locally typical sampling with parameter p.

               Locally typical sampling promotes the generation  of  contextu‚Äê
               ally coherent and diverse text by sampling tokens that are typ‚Äê
               ical  or  expected based on the surrounding context. By setting
               the parameter p between 0 and 1, you can  control  the  balance
               between  producing text that is locally coherent and diverse. A
               value closer to 1 will promote more contextually  coherent  to‚Äê
               kens,  while  a value closer to 0 will promote more diverse to‚Äê
               kens. A value equal to 1 disables locally typical sampling.

               Default: 1.0 (which means disabled)

       [1m--repeat-penalty [4m[22mN[0m
               Controls repetition of token sequences in generated text.

               This can help prevent the model from generating  repetitive  or
               monotonous text. A higher value (e.g., 1.5) will penalize repe‚Äê
               titions  more strongly, while a lower value (e.g., 0.9) will be
               more lenient.

               Default: 1.1

       [1m--repeat-last-n [4m[22mN[0m
               Last n tokens to consider for penalizing repetition.

               This controls the number of tokens in the history  to  consider
               for  penalizing  repetition.  A  larger value will look further
               back in the generated text  to  prevent  repetitions,  while  a
               smaller  value  will  only consider recent tokens. A value of 0
               disables the penalty, and a value of -1 sets the number of  to‚Äê
               kens considered equal to the context size.

               [1m-   [22m0 = disabled
               [1m-   [22m-1 = ctx_size

               Default: 64

       [1m--no-penalize-nl[0m
               Disables  penalization  of newline tokens when applying the re‚Äê
               peat penalty.

               This option is particularly useful for generating chat  conver‚Äê
               sations, dialogues, code, poetry, or any text where newline to‚Äê
               kens  play a significant role in structure and formatting. Dis‚Äê
               abling newline penalization helps maintain the natural flow and
               intended formatting in these specific use cases.

       [1m--presence-penalty [4m[22mN[0m
               Repeat alpha presence penalty.

               [1m-   [22m0.0 = disabled

               Default: 0.0

       [1m--frequency-penalty [4m[22mN[0m
               Repeat alpha frequency penalty.

               [1m-   [22m0.0 = disabled

               Default: 0.0

       [1m--mirostat [4m[22mN[0m
               Use Mirostat sampling.

               Mirostat is an algorithm that actively maintains the quality of
               generated text within a desired range during  text  generation.
               It  aims  to  strike a balance between coherence and diversity,
               avoiding low-quality  output  caused  by  excessive  repetition
               (boredom traps) or incoherence (confusion traps).

               Using Mirostat causes the Top K, Nucleus, Tail Free and Locally
               Typical samplers parameter to be ignored if used.

               [1m-   [22m0 = disabled
               [1m-   [22m1 = Mirostat
               [1m-   [22m2 = Mirostat 2.0

               Default: 0

       [1m--mirostat-lr [4m[22mN[0m
               Sets the Mirostat learning rate (eta).

               The learning rate influences how quickly the algorithm responds
               to feedback from the generated text. A lower learning rate will
               result in slower adjustments, while a higher learning rate will
               make the algorithm more responsive.

               Default: 0.1

       [1m--mirostat-ent [4m[22mN[0m
               Sets the Mirostat target entropy (tau).

               This  represents the desired perplexity value for the generated
               text.  Adjusting the target entropy allows you to  control  the
               balance  between coherence and diversity in the generated text.
               A lower value will result in more focused  and  coherent  text,
               while  a higher value will lead to more diverse and potentially
               less coherent text.

               Default: 5.0

       [1m-l [4m[22mTOKEN_ID(+/-)BIAS[24m, [1m--logit-bias [4m[22mTOKEN_ID(+/-)BIAS[0m
               Modifies the likelihood of token appearing in  the  completion,
               i.e.   [1m--logit-bias  [4m[22m15043+1[24m  to  increase  likelihood of token
               [4m'[24m [4mHello'[24m, or [1m--logit-bias [4m[22m15043-1[24m to decrease likelihood of to‚Äê
               ken [4m'[24m [4mHello'[24m.

       [1m--cfg-negative-prompt [4m[22mPROMPT[0m
               Negative prompt to use for guidance..

               Default: empty

       [1m--cfg-negative-prompt-file [4m[22mFNAME[0m
               Negative prompt file to use for guidance.

               Default: empty

       [1m--cfg-scale [4m[22mN[0m
               Strength of guidance.

               [1m-   [22m1.0 = disable

               Default: 1.0

       [1m--rope-scaling [4m[22m{none,linear,yarn}[0m
               RoPE frequency scaling method, defaults to linear unless speci‚Äê
               fied by the model

       [1m--rope-scale [4m[22mN[0m
               RoPE context scaling factor, expands context by a factor  of  [4mN[0m
               where  [4mN[24m  is  the  linear scaling factor used by the fine-tuned
               model. Some fine-tuned models have extended the context  length
               by scaling RoPE. For example, if the original pre-trained model
               have  a  context  length (max sequence length) of 4096 (4k) and
               the fine-tuned model have 32k. That is a scaling factor  of  8,
               and  should work by setting the above [1m--ctx-size [22mto 32768 (32k)
               and [1m--rope-scale [22mto 8.

       [1m--rope-freq-base [4m[22mN[0m
               RoPE base frequency, used by NTK-aware scaling.

               Default: loaded from model

       [1m--rope-freq-scale [4m[22mN[0m
               RoPE frequency scaling factor, expands context by a  factor  of
               1/N

       [1m--yarn-orig-ctx [4m[22mN[0m
               YaRN: original context size of model.

               Default: 0 = model training context size

       [1m--yarn-ext-factor [4m[22mN[0m
               YaRN: extrapolation mix factor.

               [1m-   [22m0.0 = full interpolation

               Default: 1.0

       [1m--yarn-attn-factor [4m[22mN[0m
               YaRN: scale sqrt(t) or attention magnitude.

               Default: 1.0

       [1m--yarn-beta-slow [4m[22mN[0m
               YaRN: high correction dim or alpha.

               Default: 1.0

       [1m--yarn-beta-fast [4m[22mN[0m
               YaRN: low correction dim or beta.

               Default: 32.0

       [1m--temp [4m[22mN[0m
               Adjust the randomness of the generated text.

               Temperature is a hyperparameter that controls the randomness of
               the  generated text. It affects the probability distribution of
               the model's output tokens. A  higher  temperature  (e.g.,  1.5)
               makes  the  output more random and creative, while a lower tem‚Äê
               perature (e.g., 0.5) makes the output more focused, determinis‚Äê
               tic, and conservative. The default value is 0.8, which provides
               a balance between randomness and determinism. At the extreme, a
               temperature of 0 will always pick the most likely  next  token,
               leading to identical outputs in each run.

               Default: 0.8 in cli and server mode, and 0.0 in chat mode

       [1m--logits-all[0m
               Return logits for all tokens in the batch.

               Default: disabled

       [1m-ns [4m[22mN[24m, [1m--sequences [4m[22mN[0m
               Number of sequences to decode.

               Default: 1

       [1m-pa [4m[22mN[24m, [1m--p-accept [4m[22mN[0m
               speculative decoding accept probability.

               Default: 0.5

       [1m-ps [4m[22mN[24m, [1m--p-split [4m[22mN[0m
               Speculative decoding split probability.

               Default: 0.1

       [1m--mlock[0m
               Force  system to keep model in RAM rather than swapping or com‚Äê
               pressing.

       [1m--no-mmap[0m
               Do not memory-map model (slower load but may reduce pageouts if
               not using mlock).

       [1m--numa  [22mAttempt optimizations that help on some  NUMA  systems  if  run
               without  this  previously, it is recommended to drop the system
               page       cache       before       using       this.       See
               https://github.com/ggerganov/llama.cpp/issues/1437.

       [1m--recompile[0m
               Force GPU support to be recompiled at runtime if possible.

       [1m--nocompile[0m
               Never compile GPU support at runtime.

               If  the appropriate DSO file already exists under [4m~/.llamafile/[0m
               then it'll be linked as-is without question. If a prebuilt  DSO
               is  present  in the PKZIP content of the executable, then it'll
               be extracted and linked if possible. Otherwise, [1mllamafile  [22mwill
               skip any attempt to compile GPU support and simply fall back to
               using CPU inference.

       [1m-sm [4m[22mSPLIT_MODE[24m, [1m--split-mode [4m[22mSPLIT_MODE[0m
               How to split the model across multiple GPUs, one of:
               [1m-   [22mnone: use one GPU only
               [1m-   [22mlayer (default): split layers and KV across GPUs
               [1m-   [22mrow: split rows across GPUs

       [1m-ts [4m[22mSPLIT[24m, [1m--tensor-split [4m[22mSPLIT[0m
               When using multiple GPUs this option controls how large tensors
               should  be  split  across all GPUs.  [4mSPLIT[24m is a comma-separated
               list of non-negative values that assigns the proportion of data
               that each GPU should get in order. For example, "3,2" will  as‚Äê
               sign  60% of the data to GPU 0 and 40% to GPU 1. By default the
               data is split in proportion to VRAM but this may not be optimal
               for performance. Requires cuBLAS.  How to split tensors  across
               multiple GPUs, comma-separated list of proportions, e.g. 3,1

       [1m-mg [4m[22mi[24m, [1m--main-gpu [4m[22mi[0m
               The GPU to use for scratch and small tensors.

       [1m--verbose-prompt[0m
               Print prompt before generation.

       [1m--lora [4m[22mFNAME[0m
               Apply LoRA adapter (implies [1m--no-mmap[22m)

       [1m--lora-scaled [4m[22mFNAME[24m [4mS[0m
               Apply  LoRA  adapter  with  user  defined  scaling  S  (implies
               [1m--no-mmap[22m)

       [1m--lora-base [4m[22mFNAME[0m
               Optional model to use as a base for the layers modified by  the
               LoRA adapter

       [1m--unsecure[0m
               Disables pledge() sandboxing on Linux and OpenBSD.

       [1m--samplers[0m
               Samplers  that  will be used for generation in the order, sepa‚Äê
               rated    by    semicolon,    for    example:    top_k;tfs;typi‚Äê
               cal;top_p;min_p;temp

       [1m--samplers-seq[0m
               Simplified sequence for samplers that will be used.

       [1m-dkvc[22m, [1m--dump-kv-cache[0m
               Verbose print of the KV cache.

       [1m-nkvo[22m, [1m--no-kv-offload[0m
               Disable KV offload.

       [1m-gan [4m[22mN[24m, [1m--grp-attn-n [4m[22mN[0m
               Group-attention factor.

               Default: 1

       [1m-gaw [4m[22mN[24m, [1m--grp-attn-w [4m[22mN[0m
               Group-attention width.

               Default: 512

       [1m-bf [4m[22mFNAME[24m, [1m--binary-file [4m[22mFNAME[0m
               Binary file containing multiple choice tasks.

       [1m--multiple-choice[0m
               Compute  multiple  choice score over random tasks from datafile
               supplied by the [1m-f [22mflag.

       [1m--multiple-choice-tasks [4m[22mN[0m
               Number of tasks to  use  when  computing  the  multiple  choice
               score.

               Default: 0

       [1m--kl-divergence[0m
               Computes    KL-divergence    to   logits   provided   via   the
               [1m--kl-divergence-base [22mflag.

       [1m--save-all-logits [4m[22mFNAME[24m, [1m--kl-divergence-base [4m[22mFNAME[0m
               Save logits to filename.

       [1m-ptc [4m[22mN[24m, [1m--print-token-count [4m[22mN[0m
               Print token count every [4mN[24m tokens.

               Default: -1

       [1m--pooling [4m[22mKIND[0m
               Specifies pooling type for embeddings. This may be one of:

               [1m-   [22mnone
               [1m-   [22mmean
               [1m-   [22mcls

               The model default is used if unspecified.

[1mCHAT OPTIONS[0m
       The following options may be specified when  [1mllamafile  [22mis  running  in
       [1m--chat [22mmode.

       [1m-p [4m[22mSTRING[24m, [1m--prompt [4m[22mSTRING[0m
               Specifies system prompt.

               The  system  prompt  is used to give instructions to the LLM at
               the beginning of the conversation.  For  many  model  architec‚Äê
               tures,  this  is  done  under a special role. The system prompt
               also gets special treatment when managing the  context  window.
               For  example,  the  /clear command will erase everything except
               the system prompt, and the /forget command will erase the  old‚Äê
               est chat message that isn't the system prompt.

               For example:

                     [1mllamafile  --chat  -m  model.gguf  -p  "You  are Mosaic's[0m
                     [1mGodzilla."[0m

               may be used to instruct your llamafile to roleplay as Mozilla.

       [1m-f [4m[22mFNAME[24m, [1m--file [4m[22mFNAME[0m
               Uses content of file as system prompt.

       [1m--no-display-prompt[22m, [1m--silent-prompt[0m
               Suppress printing of system prompt at  beginning  of  conversa‚Äê
               tion.

       [1m--nologo[0m
               Disables printing the llamafile logo during chatbot startup.

       [1m--ascii[0m
               This  flag  may  be  used in [1m--chat [22mmode to print the llamafile
               logo in ASCII rather than UNICODE.

       [1m--verbose[0m
               Enables verbose logger output in chatbot. This can  be  helpful
               for troubleshooting issues.

       [1m--chat-template [4m[22mNAME[0m
               Specifies or overrides chat template for model.

               Normally the GGUF metadata tokenizer.chat_template will specify
               this value for instruct models. This flag may be used to either
               override  the chat template, or specify one when the GGUF meta‚Äê
               data field is absent, which effectively forces the  web  ui  to
               enable chatbot mode.

               Supported chat template names are: chatml, llama2, llama3, mis‚Äê
               tral  (alias  for llama2), phi3, zephyr, monarch, gemma, gemma2
               (alias  for  gemma),  orion,  openchat,  vicuna,   vicuna-orca,
               deepseek, command-r, chatglm3, chatglm4, minicpm, deepseek2, or
               exaone3.

               It  is also possible to pass the jinja2 template itself to this
               argument.  Since llamafiler doesn't currently support jinja2, a
               heuristic will be used to guess which of  the  above  templates
               the template represents.

[1mCLI OPTIONS[0m
       The  following  options  may  be specified when [1mllamafile [22mis running in
       [1m--cli [22mmode.

       [1m-p [4m[22mSTRING[24m, [1m--prompt [4m[22mSTRING[0m
               Prompt to start text generation. Your LLM  works  by  auto-com‚Äê
               pleting this text. For example:

                     [1mllamafile -m model.gguf -p "four score and"[0m

               Stands  a  pretty  good chance of printing Lincoln's Gettysburg
               Address.  Prompts can take on a structured format too.  Depend‚Äê
               ing  on  how your model was trained, it may specify in its docs
               an instruction notation. With some models that might be:

                     [1mllamafile -p "[INST]Summarize this: $(cat file)[/INST]"[0m

               In most cases, simply colons and newlines will work too:

                     [1mllamafile -e -p "User: What is best in life?\nAssistant:"[0m

       [1m-f [4m[22mFNAME[24m, [1m--file [4m[22mFNAME[0m
               Prompt file to start generation.

       [1m-n [4m[22mN[24m, [1m--n-predict [4m[22mN[0m
               Sets number of tokens to predict when generating text.

               This option controls the number of tokens the  model  generates
               in  response  to the input prompt. By adjusting this value, you
               can influence the length of the generated text. A higher  value
               will  result  in  longer text, while a lower value will produce
               shorter text.

               A value of -1 will enable infinite text generation, even though
               we have a finite context window. When  the  context  window  is
               full,  some  of  the  earlier  tokens (half of the tokens after
               [1m--n-keep[22m) will be discarded. The context must then be re-evalu‚Äê
               ated before generation can resume. On large models and/or large
               context windows, this will result in significant pause in  out‚Äê
               put.

               If the pause is undesirable, a value of -2 will stop generation
               immediately when the context is filled.

               It  is important to note that the generated text may be shorter
               than the specified number of tokens if an End-of-Sequence (EOS)
               token or a reverse prompt is encountered. In  interactive  mode
               text  generation will pause and control will be returned to the
               user. In non-interactive mode, the program will  end.  In  both
               cases,  the text generation may stop before reaching the speci‚Äê
               fied `n-predict` value. If you want the  model  to  keep  going
               without  ever producing End-of-Sequence on its own, you can use
               the [1m--ignore-eos [22mparameter.

               [1m-   [22m-1 = infinity
               [1m-   [22m-2 = until context filled

               Default: -1

       [1m--simple-io[0m
               Use basic IO for better compatibility in subprocesses and  lim‚Äê
               ited consoles.

       [1m-cml[22m, [1m--chatml[0m
               Run in chatml mode (use with ChatML-compatible models)

       [1m-e[22m, [1m--escape[0m
               Process prompt escapes sequences (\n, \r, \t, \¬¥, \", \\)

       [1m--grammar [4m[22mGRAMMAR[0m
               BNF-like grammar to constrain which tokens may be selected when
               generating text. For example, the grammar:

                     [1mroot ::= "yes" | "no"[0m

               will  force  the  LLM  to only output yes or no before exiting.
               This is useful for shell scripts when  the  [1m--no-display-prompt[0m
               flag is also supplied.

       [1m--grammar-file [4m[22mFNAME[0m
               File to read grammar from.

       [1m--fast  [22mPut  llamafile  into  fast  math mode. This disables algorithms
               that reduce floating point rounding, e.g. Kahan summation,  and
               certain functions like expf() will be vectorized but handle un‚Äê
               derflows  less  gracefully.  It's unspecified whether llamafile
               runs in fast or precise math mode when neither flag  is  speci‚Äê
               fied.

       [1m--precise[0m
               Put  llamafile  into precise math mode. This enables algorithms
               that reduce floating point rounding, e.g. Kahan summation,  and
               certain  functions  like  expf()  will always handle subnormals
               correctly. It's unspecified whether llamafile runs in  fast  or
               precise math mode when neither flag is specified.

       [1m--trap  [22mPut  llamafile into math trapping mode. When floating point ex‚Äê
               ceptions occur, such as NaNs, overflow,  and  divide  by  zero,
               llamafile  will  print  a  warning to the console. This warning
               will include a C++ backtrace the first  time  an  exception  is
               trapped.  The  op graph will also be dumped to a file, and lla‚Äê
               mafile will report the specific  op  where  the  exception  oc‚Äê
               curred.  This  is useful for troubleshooting when reporting is‚Äê
               sues.  USing this feature will disable sandboxing.  Math  trap‚Äê
               ping  is  only possible if your CPU supports it. That is gener‚Äê
               ally the case on AMD64, however it's less common on ARM64.

       [1m--prompt-cache [4m[22mFNAME[0m
               File to cache prompt state for faster startup.

               Default: none

       [1m-fa [4m[22mFNAME[24m, [1m--flash-attn[0m
               Enable Flash Attention. This is a  mathematical  shortcut  that
               can  speed  up  inference  for  certain models. This feature is
               still under active development.

       [1m--prompt-cache-all[0m
               If specified, saves user input  and  generations  to  cache  as
               well. Not supported with [1m--interactive [22mor other interactive op‚Äê
               tions.

       [1m--prompt-cache-ro[0m
               If specified, uses the prompt cache but does not update it.

       [1m--random-prompt[0m
               Start with a randomized prompt.

       [1m--image [4m[22mIMAGE_FILE[0m
               Path to an image file. This should be used with multimodal mod‚Äê
               els.   Alternatively,  it's possible to embed an image directly
               into the prompt instead; in which case, it must be  base64  en‚Äê
               coded  into  an HTML img tag URL with the image/jpeg MIME type.
               See also the [1m--mmproj [22mflag for supplying the vision model.

       [1m-i[22m, [1m--interactive[0m
               Run the program in interactive mode, allowing users  to  engage
               in  real-time conversations or provide specific instructions to
               the model.

       [1m--interactive-first[0m
               Run the program in interactive mode and  immediately  wait  for
               user input before starting the text generation.

       [1m-ins[22m, [1m--instruct[0m
               Run  the program in instruction mode, which is specifically de‚Äê
               signed to work with Alpaca  models  that  excel  in  completing
               tasks based on user instructions.

               Technical details: The user's input is internally prefixed with
               the  reverse prompt (or "### Instruction:" as the default), and
               followed by "### Response:" (except if you  just  press  Return
               without any input, to keep generating a longer response).

               By  understanding  and utilizing these interaction options, you
               can create engaging and dynamic experiences with the LLaMA mod‚Äê
               els, tailoring the text generation  process  to  your  specific
               needs.

       [1m-r [4m[22mPROMPT[24m, [1m--reverse-prompt [4m[22mPROMPT[0m
               Specify  one  or multiple reverse prompts to pause text genera‚Äê
               tion and switch to interactive mode. For  example,  [1m-r  [4m[22m"User:"[0m
               can  be  used  to jump back into the conversation whenever it's
               the user's turn to speak. This helps create a more  interactive
               and  conversational  experience.  However,  the  reverse prompt
               doesn't work when it ends with a space. To overcome this  limi‚Äê
               tation,  you can use the [1m--in-prefix [22mflag to add a space or any
               other characters after the reverse prompt.

       [1m--color[0m
               Enable colorized output to differentiate visually  distinguish‚Äê
               ing between prompts, user input, and generated text.

       [1m--no-display-prompt[22m, [1m--silent-prompt[0m
               Don't echo the prompt itself to standard output.

       [1m--keep [4m[22mN[0m
               Specifies number of tokens to keep from the initial prompt. The
               default is -1 which means all tokens.

       [1m--multiline-input[0m
               Allows you to write or paste multiple lines without ending each
               in '\'.

       [1m--cont-batching[0m
               Enables  continuous  batching,  a.k.a. dynamic batching.  is -1
               which means all tokens.

       [1m--embedding[0m
               In CLI mode, the embedding flag may be use to print  embeddings
               to  standard output. By default, embeddings are computed over a
               whole prompt. However the [1m--multiline [22mflag may  be  passed,  to
               have a separate embeddings array computed for each line of text
               in  the prompt. In multiline mode, each embedding array will be
               printed on its own line to standard  output,  where  individual
               floats  are  separated  by space. If both the [1m--multiline-input[0m
               and [1m--interactive [22mflags are passed, then a pretty-printed  sum‚Äê
               mary  of  embeddings along with a cosine similarity matrix will
               be printed to the terminal.

       [1m--ignore-eos[0m
               Ignore end of stream token  and  continue  generating  (implies
               [1m--logit-bias [4m[22m2-inf[24m)

       [1m--keep [4m[22mN[0m
               This  flag  allows users to retain the original prompt when the
               model runs out of context, ensuring a connection to the initial
               instruction or conversation topic is maintained, where [4mN[24m is the
               number of tokens from the initial prompt  to  retain  when  the
               model resets its internal context.

               [1m-   [22m0 = no tokens are kept from initial prompt
               [1m-   [22m-1 = retain all tokens from initial prompt

               Default: 0

       [1m--in-prefix-bos[0m
               Prefix BOS to user inputs, preceding the [1m--in-prefix [22mstring.

       [1m--in-prefix [4m[22mSTRING[0m
               This  flag  is  used  to add a prefix to your input, primarily,
               this is used to insert a space after the reverse prompt. Here's
               an example of how to use the [1m--in-prefix  [22mflag  in  conjunction
               with the [1m--reverse-prompt [22mflag:

                     [1m./main -r "User:" --in-prefix " "[0m

               Default: empty

       [1m--in-suffix [4m[22mSTRING[0m
               This  flag  is  used  to add a suffix after your input. This is
               useful for adding an "Assistant:" prompt after the  user's  in‚Äê
               put.  It's added after the new-line character (\n) that's auto‚Äê
               matically added to the end of the user's input. Here's an exam‚Äê
               ple of how to use the [1m--in-suffix [22mflag in conjunction with  the
               [1m--reverse-prompt [22mflag:

                     [1m./main   -r   "User:"   --in-prefix   "   "   --in-suffix[0m
                     [1m"Assistant:"[0m

               Default: empty

[1mSERVER OPTIONS[0m
       The following options may be specified when  [1mllamafile  [22mis  running  in
       [1m--server [22mmode.

       [1m--port [4m[22mPORT[0m
               Port to listen

               Default: 8080

       [1m--host [4m[22mIPADDR[0m
               IP address to listen.

               Default: 127.0.0.1

       [1m-to [4m[22mN[24m, [1m--timeout [4m[22mN[0m
               Server read/write timeout in seconds.

               Default: 600

       [1m-np [4m[22mN[24m, [1m--parallel [4m[22mN[0m
               Number of slots for process requests.

               Default: 1

       [1m-cb[22m, [1m--cont-batching[0m
               Enable continuous batching (a.k.a dynamic batching).

               Default: disabled

       [1m-spf [4m[22mFNAME[24m, [1m--system-prompt-file [4m[22mFNAME[0m
               Set  a  file  to  load  a  system prompt (initial prompt of all
               slots), this is useful for chat applications.

       [1m-a [4m[22mALIAS[24m, [1m--alias [4m[22mALIAS[0m
               Set an alias for the model. This will be  added  as  the  [4mmodel[0m
               field in completion responses.

       [1m--path [4m[22mPUBLIC_PATH[0m
               Path from which to serve static files.

               Default: [4m/zip/llama.cpp/server/public[0m

       [1m--url-prefix [4m[22mPREFIX[0m
               Specify a URL prefix (subdirectory) under which the API will be
               served, e.g. /llamafile

               Default: [4m/[0m

       [1m--nobrowser[0m
               Do not attempt to open a web browser tab at startup.

       [1m-gan [4m[22mN[24m, [1m--grp-attn-n [4m[22mN[0m
               Set  the  group attention factor to extend context size through
               self-extend. The default value is [4m1[24m which means disabled.  This
               flag is used together with [1m--grp-attn-w[22m.

       [1m-gaw [4m[22mN[24m, [1m--grp-attn-w [4m[22mN[0m
               Set  the  group  attention width to extend context size through
               self-extend. The default value is [4m512[24m.  This flag is  used  to‚Äê
               gether with [1m--grp-attn-n[22m.

[1mLOG OPTIONS[0m
       The following log options are available:

       [1m-ld [4m[22mLOGDIR[24m, [1m--logdir [4m[22mLOGDIR[0m
               Path under which to save YAML logs (no logging if unset)

       [1m--log-test[0m
               Run simple logging test

       [1m--log-disable[0m
               Disable trace logs

       [1m--log-enable[0m
               Enable trace logs

       [1m--log-file[0m
               Specify a log filename (without extension)

       [1m--log-new[0m
               Create  a  separate  new  log file on start. Each log file will
               have unique name: [4m<name>.<ID>.log[0m

       [1m--log-append[0m
               Don't truncate the old log file.

[1mEXAMPLES[0m
       Here's an example of how to run llama.cpp's built-in HTTP server.  This
       example   uses   LLaVA  v1.5-7B,  a  multimodal  LLM  that  works  with
       llama.cpp's recently-added support for image inputs.

             llamafile \
               -m llava-v1.5-7b-Q8_0.gguf \
               --mmproj llava-v1.5-7b-mmproj-Q8_0.gguf \
               --host 0.0.0.0

       Here's an example of how to generate code for a libc function using the
       llama.cpp  command  line  interface,  utilizing  WizardCoder-Python-13B
       weights:

             llamafile \
               -m wizardcoder-python-13b-v1.0.Q8_0.gguf --temp 0 -r '}\n' -r '```\n' \
               -e -p '```c\nvoid *memcpy(void *dst, const void *src, size_t size) {\n'

       Here's  a  similar  example  that  instead utilizes Mistral-7B-Instruct
       weights for prose composition:

             llamafile \
               -m mistral-7b-instruct-v0.2.Q5_K_M.gguf \
               -p '[INST]Write a story about llamas[/INST]'

       Here's an example of how llamafile can be used as an interactive  chat‚Äê
       bot that lets you query knowledge contained in training data:

             llamafile -m llama-65b-Q5_K.gguf -p '
             The following is a conversation between a Researcher and their helpful AI
             assistant Digital Athena which is a large language model trained on the
             sum of human knowledge.
             Researcher: Good morning.
             Digital Athena: How can I help you today?
             Researcher:' --interactive --color --batch_size 1024 --ctx_size 4096 \
             --keep -1 --temp 0 --mirostat 2 --in-prefix ' ' --interactive-first \
             --in-suffix 'Digital Athena:' --reverse-prompt 'Researcher:'

       Here's an example of how you can use llamafile to summarize HTML URLs:

             (
               echo '[INST]Summarize the following text:'
               links -codepage utf-8 \
                     -force-html \
                     -width 500 \
                     -dump https://www.poetryfoundation.org/poems/48860/the-raven |
                 sed 's/   */ /g'
               echo '[/INST]'
             ) | llamafile \
                   -m mistral-7b-instruct-v0.2.Q5_K_M.gguf \
                   -f /dev/stdin \
                   -c 0 \
                   --temp 0 \
                   -n 500 \
                   --no-display-prompt 2>/dev/null

       Here's how you can use llamafile to describe a jpg/png/gif image:

             llamafile --temp 0 \
               --image lemurs.jpg \
               -m llava-v1.5-7b-Q4_K.gguf \
               --mmproj llava-v1.5-7b-mmproj-Q4_0.gguf \
               -e -p '### User: What do you see?\n### Assistant: ' \
               --no-display-prompt 2>/dev/null

       If  you  wanted  to  write a script to rename all your image files, you
       could use the following command to generate a safe filename:

             llamafile --temp 0 \
                 --image ~/Pictures/lemurs.jpg \
                 -m llava-v1.5-7b-Q4_K.gguf \
                 --mmproj llava-v1.5-7b-mmproj-Q4_0.gguf \
                 --grammar 'root ::= [a-z]+ (" " [a-z]+)+' \
                 -e -p '### User: The image has...\n### Assistant: ' \
                 --no-display-prompt 2>/dev/null |
               sed -e's/ /_/g' -e's/$/.jpg/'
             three_baby_lemurs_on_the_back_of_an_adult_lemur.jpg

       Here's an example of how to make an API request to the OpenAI API  com‚Äê
       patible  completions  endpoint  when  your  [1mllamafile [22mis running in the
       background in [1m--server [22mmode.

             curl -s http://localhost:8080/v1/chat/completions \
                  -H "Content-Type: application/json" -d '{
               "model": "gpt-3.5-turbo",
               "stream": true,
               "messages": [
                 {
                   "role": "system",
                   "content": "You are a poetic assistant."
                 },
                 {
                   "role": "user",
                   "content": "Compose a poem that explains FORTRAN."
                 }
               ]
             }' | python3 -c '
             import json
             import sys
             json.dump(json.load(sys.stdin), sys.stdout, indent=2)
             print()

[1mPROTIP[0m
       The [1m-ngl [4m[22m35[24m flag needs to be passed in order to use GPUs made by NVIDIA
       and AMD.  It's not enabled by default since it sometimes  needs  to  be
       tuned  based on the system hardware and model architecture, in order to
       achieve optimal performance, and avoid compromising a shared display.

[1mSEE ALSO[0m
       [4mllamafile-quantize[24m(1),   [4mllamafile-perplexity[24m(1),    [4mllava-quantize[24m(1),
       [4mzipalign[24m(1), [4munzip[24m(1)

Mozilla Ocho                   October 12, 2024                   [4mLLAMAFILE[24m(1)
