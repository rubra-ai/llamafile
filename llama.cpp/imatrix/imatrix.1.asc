[4mLLAMAFILE-IMATRIX[24m(1)        General Commands Manual       [4mLLAMAFILE-IMATRIX[24m(1)

[1mNAME[0m
       llamafile-imatrix ‚Äî importance matrix builder

[1mSYNOPSIS[0m
       [1mllamafile-imatrix    [22m[flags...]    [1m-m   [4m[22mmodel.gguf[24m   [1m-f   [4m[22mtraining.data[0m
                         [[1m-o [4m[22mimatrix.dat[24m]

[1mDESCRIPTION[0m
       [1mllamafile-imatrix [22mCompute an importance matrix for a  model  and  given
       text  dataset.  Can be used during quantization to enchance the quality
       of  the  quantum  models.   More   information   is   available   here:
       https://github.com/ggerganov/llama.cpp/pull/4861

[1mOPTIONS[0m
       The following options are available:

       [1m--version[0m
               Print version and exit.

       [1m-h[22m, [1m--help[0m
               Show help message and exit.

       [1m-m [4m[22mFNAME[24m, [1m--model [4m[22mFNAME[0m
               Model path in the GGUF file format.

               Default: [4mmodels/7B/ggml-model-f16.gguf[0m

       [1m-f [4m[22mFNAME[24m, [1m--file [4m[22mFNAME[0m
               Mandatory   path   of   file  containing  training  data,  e.g.
               [4mwiki.train.raw[0m

       [1m-o [4m[22mFNAME[24m, [1m--output-file [4m[22mFNAME[0m
               The name of the file where the computed data will be stored. If
               this flag is missing then [4mimatrix.dat[24m is used.

       [1m-ofreq[22m, [1m--output-frequency[0m
               Specifies how often the so far  computed  result  is  saved  to
               disk. The default is is 10 (i.e., every 10 chunks).

       [1m-ow[22m, [1m--output-weight[0m
               Specifies  if data will be collected for the [4moutput.weight[24m ten‚Äê
               sor. Experience indicates that it is better to not utilize  the
               importance matrix when quantizing [4moutput.weight[24m, so this is set
               to false by default.

       [1m--chunks [4m[22mN[0m
               Max number of chunks to process.

               [1m-   [22m-1 = all

               Default: -1

[1mPROTIPS[0m
       For faster computation, pass the [1m-ngl [4m[22m9999[24m flag for GPU offloading.

[1mSEE ALSO[0m
       [4mllamafile[24m(1), [4mllamafile-quantize[24m(1)

Llamafile Manual               December 5, 2023           [4mLLAMAFILE-IMATRIX[24m(1)
